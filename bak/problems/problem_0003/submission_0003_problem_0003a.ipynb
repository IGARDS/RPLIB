{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rankability Predicting Sensitivity\n",
    "## March Madness Dataset\n",
    "\n",
    "Goal of this notebook is to analyze and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import skew\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"%s/rankability_toolbox_dev\"%home)\n",
    "import pyrankability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,\"%s/sensitivity_study/src\"%home)\n",
    "from sensitivity_tests import *\n",
    "from utilities import *\n",
    "from base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset represents the sensitivity problem as defined as follows:\n",
    "\n",
    "A practitioner wants to predict the degree to which a tournament from the Lichess Arena Tournaments\n",
    "will be likely to change after the second half of the tournament is played. The practioner expects that rankings for traditional tournaments are more likely to change than rankings for Berserk tournaments. This is in the context of a Massey with a specific set of parameters:<br>\n",
    "direct_thress = [0,1,2]<br>\n",
    "spread_thress = [0,3,6]<br>\n",
    "weight_indirects = [0.25,0.5,1.]<br>\n",
    "domains_ranges = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = joblib.load(\"/disk/rankability_datasets/sensitivity_study/problem_0002a.joblib.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(problem[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem['data']['2002'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem['target'].to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(game_df,team_range,direct_thres,spread_thres,weight_indirect):\n",
    "    hillside_columns = [\"direct_thres\",\"spread_thres\",\"weight_indirect\",\"details\"]\n",
    "\n",
    "    map_func = lambda linked: pyrankability.construct.support_map_vectorized_direct_indirect_weighted(linked,direct_thres=direct_thres,spread_thres=spread_thres,weight_indirect=weight_indirect)\n",
    "    D = pyrankability.construct.V_count_vectorized(game_df,map_func).reindex(index=team_range,columns=team_range)\n",
    "    k,details = pyrankability.rank.solve(D,method='hillside',lazy=False,cont=True)\n",
    "    x = pd.DataFrame(details['x'],columns=D.columns,index=D.index)\n",
    "    c = pd.DataFrame(pyrankability.construct.C_count(D),columns=D.columns,index=D.index)\n",
    "    P = details['P']\n",
    "    simple_details = {'k':k,'x':x,'c':c,'P':P,'D':D}\n",
    "    hillside_ret = pd.Series([direct_thres,spread_thres,weight_indirect,simple_details],index=hillside_columns)\n",
    "    return hillside_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import kendalltau\n",
    "def score_by_correlation(s,r):\n",
    "    return pearsonr(s,r)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(y,score_by,direct_thres,spread_thres,weight_indirect,domain_range,top_n=10):\n",
    "    parameter_string = f\"{domain_range},dt={direct_thres},st={spread_thres},iw={weight_indirect}\"\n",
    "    values = []\n",
    "    for year in y.index:\n",
    "        # set the team_range\n",
    "        team_range = None\n",
    "        if domain_range[1] == 'madness':\n",
    "            team_range = madness_teams[year]\n",
    "        elif domain_range[1] == 'all':\n",
    "            team_range = all_teams[year]\n",
    "        elif \"top\" in domain_range:\n",
    "            team_range = all_teams[year]\n",
    "        \n",
    "        knorms = []\n",
    "        for frac_key in ['frac=0.5']:#problem['data'][year].keys():\n",
    "            hillside_details = compute(problem['data'][year][frac_key],team_range,direct_thres,spread_thres,weight_indirect)\n",
    "            perm = np.array(hillside_details['details']['P'][0])\n",
    "            C = hillside_details['details']['c'].iloc[perm,:].iloc[:,perm].iloc[:top_n,:].iloc[:,:top_n]\n",
    "            n = len(C)\n",
    "            kmax = (n*n-n)/2 * n\n",
    "            k = np.sum(np.triu(C))\n",
    "            knorms.append(k/kmax)\n",
    "        knorm = np.mean(knorms)\n",
    "        values.append(knorm)\n",
    "    return pd.DataFrame([[score_by(values,y),parameter_string]],columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_thress = [0,1,2]\n",
    "spread_thress = [0,3,6]\n",
    "weight_indirects = [0.25,0.5,1.]\n",
    "domains_ranges = [('all','madness'),('madness','madness')]\n",
    "\n",
    "keys = list(itertools.product(direct_thress,spread_thress,weight_indirects,domains_ranges))\n",
    "\n",
    "test_results = pd.DataFrame(columns=[\"Score\",\"Parameters\"]).set_index('Parameters')\n",
    "for direct_thres,spread_thres,weight_indirect,domain_range in keys:\n",
    "    targets = problem['target'].to_frame().reset_index().set_index(['direct_thres','spread_thres','weight_indirect','Domain','Range']).loc[direct_thres,spread_thres,weight_indirect,domain_range[0],domain_range[1]]\n",
    "    targets = targets.set_index('Year')['intersection_0.5_to_1.0']\n",
    "\n",
    "    years = list(targets.index)\n",
    "    remaining_games = problem['other']['remaining_games']\n",
    "    madness_teams = problem['other']['madness_teams']\n",
    "\n",
    "    years_train = ['2002','2003','2004','2005','2006']\n",
    "    years_test = copy.copy(years)\n",
    "    for year in years_train:\n",
    "        years_test.remove(year)\n",
    "\n",
    "    test_results1 = calc_score(targets.loc[years_test],score_by_correlation,direct_thres,spread_thres,weight_indirect,domain_range)\n",
    "    test_results = test_results.append(test_results1.set_index('Parameters'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.sort_values(by=\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
